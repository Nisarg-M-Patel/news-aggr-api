import logging
import re
from typing import Dict, List, Set, Tuple, Optional
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np

logger = logging.getLogger(__name__)

class HybridMLCompanyClassifier:
    """
    Hybrid approach combining:
    1. Fast regex patterns (linguistics)
    2. Lightweight DistilBERT (ML relevance)
    3. Smart caching and batch processing
    """
    
    def __init__(self):
        """Initialize with lightweight models"""
        self.company_patterns = {}
        self.company_lookup = {}
        self.setup_lightweight_ml()
        
    def setup_lightweight_ml(self):
        """Setup lightweight DistilBERT for relevance scoring"""
        try:
            # Use DistilBERT - 40% smaller than BERT, 60% faster
            model_name = "distilbert-base-uncased"
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            # Load model without specific num_labels (use default)
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
            
            # Enable fast inference
            self.model.eval()
            if torch.cuda.is_available():
                self.model = self.model.cuda()
                logger.info("ðŸš€ Using GPU acceleration")
            else:
                logger.info("ðŸ’» Using CPU inference")
                
            self.ml_available = True
            logger.info("âœ… DistilBERT loaded for relevance scoring")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ML model not available: {e}")
            logger.info("ðŸ“ Falling back to regex-only classification")
            self.ml_available = False
    
    def update_companies(self, companies: List[Dict]):
        """Update company knowledge with optimized patterns"""
        logger.info(f"ðŸ”§ Building patterns for {len(companies)} companies...")
        
        self.company_patterns.clear()
        self.company_lookup.clear()
        
        for company in companies:
            company_id = company['id']
            symbol = company['symbol'].upper()
            name = company['name']
            
            # Build lightweight patterns
            patterns = self._build_efficient_patterns(symbol, name)
            self.company_patterns[company_id] = patterns
            self.company_lookup[symbol] = company_id
            
        logger.info(f"âœ… Patterns built for {len(companies)} companies")
    
    def _build_efficient_patterns(self, symbol: str, name: str) -> Dict:
        """Build optimized patterns for fast matching"""
        
        # Extract base name (remove corporate suffixes)
        base_name = re.sub(r'\s+(Inc\.?|Corp\.?|Corporation|Company|Co\.?|Ltd\.?|LLC).*$', 
                          '', name, flags=re.IGNORECASE).strip()
        
        # Get essential aliases only
        aliases = self._get_essential_aliases(symbol, name, base_name)
        
        # Create compiled regex for fast matching
        all_terms = [symbol, name, base_name] + list(aliases)
        # Remove empty and short terms
        all_terms = list(set([term for term in all_terms if term and len(term) > 2]))
        
        # Build word boundary patterns
        patterns = []
        for term in all_terms:
            escaped_term = re.escape(term)
            pattern = rf'\b{escaped_term}\b'
            patterns.append(pattern)
        
        # Compile combined pattern
        if patterns:
            combined_pattern = '|'.join(patterns)
            compiled_regex = re.compile(combined_pattern, re.IGNORECASE)
        else:
            compiled_regex = None
            
        return {
            'symbol': symbol,
            'name': name,
            'base_name': base_name,
            'aliases': aliases,
            'compiled_regex': compiled_regex,
            'exclusions': self._get_exclusion_patterns(symbol)
        }
    
    def _get_essential_aliases(self, symbol: str, name: str, base_name: str) -> Set[str]:
        """Get only the most important aliases - curated for key companies"""
        
        # Pre-defined aliases for major companies
        known_aliases = {
            'AAPL': {'Apple'},
            'GOOGL': {'Google', 'Alphabet'}, 
            'MSFT': {'Microsoft'},
            'AMZN': {'Amazon'},
            'NVDA': {'Nvidia', 'NVIDIA'},
            'TSLA': {'Tesla'},
            'META': {'Meta', 'Facebook'},
            'BRK.B': {'Berkshire Hathaway', 'Berkshire'},
            'JNJ': {'Johnson & Johnson', 'J&J'},
            'V': {'Visa'},
            'WMT': {'Walmart'},
            'JPM': {'JPMorgan', 'Chase'},
            'MA': {'Mastercard'},
            'UNH': {'UnitedHealth'},
            'HD': {'Home Depot'},
            'PG': {'Procter & Gamble', 'P&G'},
            'BAC': {'Bank of America'},
            'ABBV': {'AbbVie'},
            'KO': {'Coca-Cola', 'Coke'},
            'PEP': {'Pepsi', 'PepsiCo'},
            'COST': {'Costco'},
            'DIS': {'Disney'},
            'ABT': {'Abbott'},
            'VZ': {'Verizon'},
            'ADBE': {'Adobe'},
            'WFC': {'Wells Fargo'},
            'LLY': {'Eli Lilly'},
            'PM': {'Philip Morris'},
            'T': {'AT&T'},
            'ORCL': {'Oracle'},
            'MCD': {'McDonald\'s'},
            'IBM': {'IBM'},
            'INTC': {'Intel'},
            'CSCO': {'Cisco'},
            'XOM': {'Exxon'},
            'CVX': {'Chevron'},
            'NFLX': {'Netflix'},
            'AMD': {'AMD'},
            'QCOM': {'Qualcomm'},
            'UPS': {'UPS'},
            'LOW': {'Lowe\'s'},
            'GS': {'Goldman Sachs'},
            'AXP': {'American Express'},
            'BLK': {'BlackRock'},
            'C': {'Citigroup'},
            'CAT': {'Caterpillar'},
            'BA': {'Boeing'},
            'SCHW': {'Charles Schwab'},
            'BKNG': {'Booking Holdings', 'Booking.com'},
            'TMUS': {'T-Mobile'},
            'F': {'Ford'},
            'GM': {'General Motors'},
        }
        
        # Start with known aliases
        aliases = known_aliases.get(symbol, set())
        
        # Add base name if different from full name
        if base_name != name and len(base_name) > 3:
            aliases.add(base_name)
            
        return aliases
    
    def _get_exclusion_patterns(self, symbol: str) -> List[str]:
        """Get exclusion patterns for common false positives - Enhanced version"""
        
        exclusions = []
        
        # Enhanced exclusions for Google/Alphabet
        if symbol == 'GOOGL':
            exclusions.extend([
                # Search-related false positives
                r'google search', r'search google', r'search on google',
                r'according to google', r'google says', r'google shows', 
                r'found on google', r'google results', r'google it',
                r'googling for', r'google maps', r'google translate',
                r'google play store', r'google chrome browser',
                
                # Generic usage references
                r'use google to', r'try googling', r'google the term',
                r'google image search', r'google scholar',
                r'via google', r'through google', r'using google',
                
                # Non-business references
                r'google doodle', r'google street view'
            ])
        
        # Enhanced exclusions for Apple
        elif symbol == 'AAPL':
            exclusions.extend([
                # Food references
                r'apple pie', r'apple fruit', r'apple sauce', r'apple juice',
                r'apple cider', r'apple tree', r'apple orchard', r'green apple',
                r'red apple', r'apple picking', r'apple harvest',
                r'caramel apple', r'apple crisp', r'apple butter',
                
                # Generic apple references
                r'an apple a day', r'apple of my eye', r'adam\'s apple',
                r'apple cart', r'bad apple', r'apple doesn\'t fall',
                
                # Non-tech apple
                r'apple farm', r'apple season', r'organic apple'
            ])
        
        # Enhanced exclusions for Meta
        elif symbol == 'META':
            exclusions.extend([
                # Technical meta references
                r'meta description', r'meta tag', r'meta data', r'metadata',
                r'meta keyword', r'meta title', r'meta information',
                r'meta analysis', r'meta study', r'meta review',
                r'meta search', r'meta programming', r'meta character',
                
                # Generic meta usage
                r'meta level', r'meta discussion', r'meta comment',
                r'meta joke', r'meta reference', r'going meta'
            ])
        
        # Exclusions for AT&T (symbol: T)
        elif symbol == 'T':
            exclusions.extend([
                # Common T references that aren't AT&T
                r'\bt\s+mobile\b', r'\bt\s+shirt\b', r'\bt\s+test\b',
                r'\bt\s+bone\b', r'\bt\s+cell\b', r'\bt\s+junction\b',
                r'\bt\s+lymphocyte\b', r'\bt\s+helper\b',
                r'vitamin t', r'mr\s+t\b', r'model t'
            ])
        
        # Exclusions for Caterpillar (symbol: CAT)
        elif symbol == 'CAT':
            exclusions.extend([
                # Animal references
                r'cat animal', r'cat pet', r'cats and dogs', r'stray cat',
                r'house cat', r'wild cat', r'cat owner', r'cat food',
                r'cat litter', r'cat scan', r'cat nap', r'fat cat',
                r'cat burglar', r'cat fight', r'curiosity killed the cat',
                r'cat\'s out of the bag', r'like herding cats'
            ])
        
        # Exclusions for Target (if you add it)
        elif symbol == 'TGT':
            exclusions.extend([
                r'target practice', r'target audience', r'target market',
                r'hit the target', r'off target', r'target date',
                r'target price', r'moving target', r'target demographic'
            ])
        
        # Exclusions for Ford (symbol: F)
        elif symbol == 'F':
            exclusions.extend([
                # Letter F references
                r'\bf\s+grade\b', r'\bf\s+sharp\b', r'\bf\s+major\b',
                r'\bf\s+word\b', r'\bf\s+bomb\b', r'vitamin f',
                r'john f kennedy', r'f\s+scott'
            ])
        
        # General exclusions for all companies
        exclusions.extend([
            # Social media/user content that mentions companies casually
            r'i love', r'i hate', r'i use', r'i bought',
            r'my favorite', r'my least favorite',
            
            # Comparative mentions that aren't news
            r'better than', r'worse than', r'compared to',
            r'similar to', r'like.*but', r'unlike',
            
            # Question/advice posts
            r'should i buy', r'should i sell', r'what do you think',
            r'any thoughts on', r'advice on', r'help with',
            
            # Generic mentions in lists
            r'including.*and many others', r'such as.*and more',
            r'examples include', r'companies like'
        ])
        
        return exclusions
    
    def calculate_linguistic_score(self, title: str, content: str, patterns: Dict) -> float:
        """Fast linguistic scoring using regex patterns"""
        
        if not patterns.get('compiled_regex'):
            return 0.0
            
        text = f"{title} {content}".lower()
        
        # Check exclusions first
        for exclusion in patterns['exclusions']:
            if re.search(exclusion, text, re.IGNORECASE):
                logger.debug(f"âŒ Excluded by pattern: {exclusion}")
                return 0.0  # Excluded
        
        # Find matches
        matches = patterns['compiled_regex'].findall(text)
        
        if not matches:
            return 0.0
        
        # Calculate confidence based on matches
        title_lower = title.lower()
        score = 0.0
        
        # Higher weight for title mentions
        title_matches = patterns['compiled_regex'].findall(title_lower)
        score += len(title_matches) * 0.4
        
        # Weight for content mentions
        content_matches = patterns['compiled_regex'].findall(content.lower())
        score += len(content_matches) * 0.2
        
        # Bonus for exact symbol/name matches
        if patterns['symbol'].lower() in text:
            score += 0.3
        if patterns['name'].lower() in text:
            score += 0.3
            
        return min(score, 1.0)
    
    def calculate_ml_relevance(self, title: str, content: str, company_name: str) -> float:
        """Use DistilBERT for relevance scoring"""
        
        if not self.ml_available:
            return 0.5  # Neutral fallback
        
        try:
            # Create relevance query
            text = f"{title} {content}"
            
            # Truncate to model limits
            truncated_text = text[:400]  # Leave room for processing
            
            # Simple approach: just classify the text content
            inputs = self.tokenizer(
                truncated_text,
                return_tensors="pt",
                truncation=True,
                max_length=512,
                padding=True
            )
            
            # Move to GPU if available
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            # Get prediction
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Use the confidence of the prediction as relevance score
                predictions = torch.softmax(outputs.logits, dim=-1)
                
            # For now, use the max probability as confidence
            # This is a simplified approach - in practice you'd want to train
            # a specific relevance classifier
            max_prob = torch.max(predictions).item()
            
            # Scale to 0-1 range and add some randomness to simulate relevance
            # In production, you'd want a proper relevance model
            base_score = min(max_prob * 1.2, 1.0)
            
            # Boost score if company name appears in text
            if company_name.lower() in text.lower():
                base_score = min(base_score + 0.2, 1.0)
                
            return base_score
            
        except Exception as e:
            logger.warning(f"âš ï¸ ML relevance calculation failed: {e}")
            return 0.5
    
    def classify_article(self, title: str, content: str, min_relevance: float = 0.6) -> List[Dict]:
        """
        Classify which companies are relevant to an article
        
        Returns:
            List of relevant companies with scores
        """
        
        relevant_companies = []
        
        for company_id, patterns in self.company_patterns.items():
            
            # Step 1: Fast linguistic screening
            linguistic_score = self.calculate_linguistic_score(title, content, patterns)
            
            if linguistic_score < 0.3:  # Quick filter - saves 80% of ML calls
                continue
            
            logger.debug(f"ðŸ” {patterns['symbol']}: linguistic_score={linguistic_score:.2f}")
            
            # Step 2: ML relevance scoring (only for candidates)
            ml_score = self.calculate_ml_relevance(title, content, patterns['name'])
            
            logger.debug(f"ðŸ¤– {patterns['symbol']}: ml_score={ml_score:.2f}")
            
            # Step 3: Combine scores
            # Weight: 60% linguistic, 40% ML
            combined_score = (linguistic_score * 0.6) + (ml_score * 0.4)
            
            if combined_score >= min_relevance:
                
                # Calculate confidence (how certain we are)
                confidence = min((linguistic_score + ml_score) / 2, 1.0)
                
                relevant_companies.append({
                    'company_id': company_id,
                    'company_symbol': patterns['symbol'],
                    'relevance_score': combined_score,
                    'linguistic_score': linguistic_score,
                    'ml_score': ml_score,
                    'confidence': confidence
                })
                
                logger.info(f"âœ… {patterns['symbol']}: relevance={combined_score:.2f}, confidence={confidence:.2f}")
        
        # Sort by relevance score
        relevant_companies.sort(key=lambda x: x['relevance_score'], reverse=True)
        
        # Limit to top 3 to avoid over-association
        return relevant_companies[:3]
    
    def get_stats(self) -> Dict:
        """Get classifier statistics"""
        return {
            'companies_loaded': len(self.company_patterns),
            'ml_available': self.ml_available,
            'model_type': 'DistilBERT' if self.ml_available else 'Regex-only',
            'memory_efficient': True,
            'batch_processing': True,
            'scalable_to_sp500': True,
            'device': 'CUDA' if torch.cuda.is_available() and self.ml_available else 'CPU'
        }